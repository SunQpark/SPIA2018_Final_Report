\chapter{Introduction}\label{Ch:Introduction}

The goal of this project is to build a system that can generate photo-realistic images from rough sketch pictures. To serve this purpose, we utilized the Cycle-GAN~\cite{CycleGAN} with sketch images collected from google image search and Celebrity A dataset~\cite{liu2015faceattributes}. The process of collecting and refining data, and detailed information about netrwork architecture are provided at Chapter~\ref{Ch:process_data} and at Chapter~\ref{Ch:Model} respectively. 

The series of experiments and their result images can be found in Chapter~\ref{Ch:Result} and Chapter~\ref{Ch:Conclusion} with description, which would be considered as the main content of this report. All the code we used has been made available to public at \texttt{https://github.com/SunQpark/SPIA2018\_cycle\_GAN}. The only thing excluded is the input images, which are considered to contain some images that are able to cause some legal issues. Any kind of interest or contribution on our project would be welcomed.

\section{Related Works}

Sketch-to-Photograph generation is a sub-problem of task called Image-to-Image translation. In general, the goal of the Image-to-Image problem is to learn the mapping between distinct domains of image. The problem include some of most important problems in image processing by computer science, such as image segmentation~\cite{FCN} where one domain is the natural image and the other is semantic label of that image, or image colorization~\cite{colorization} where one domain is grayscale image and the other is RGB image, and so on. The recent Image-to-Image translation approach has been hugely improved due to the raise of deep learning in the field of computer vision, especially using the method of \textbf{Generative adversarial network(GAN)} models.

Being published by Ian Goodfellow in 2014, GAN is originally developed as a generative model whose purpose is to train a generative model that can make fake data which look as similar as the given set of train data. To achive that object, GAN make use of two-player min-max game setup. One part of GAN is called the Generator, which are supposed to learn to generate images. To be more specific, generator part of GAN takes random gaussian noise vector as input and transform that noise into image features using the set of parameters included in the model. On the other hand Discriminator, the remaining part of GAN, takes image as input, and guesses whether the input image is sampled from the given set of input data(real) or generated(fake) by the generator. As the training of GAN goes on, the discriminator gets better and better at telling the fake images apart from the real images, which results in the generator to produce realistic images to fool the discriminator.

When GAN was so successful in the field of image generation, it was a model called \textbf{Pix-to-Pix}~\cite{pix2pix} that first succeeded in applying GAN to image conversion, not image creation. While there were approaches before Pix-to-Pix to transform images using supervised deep-learning, with the ordinary cross-entropy or mean-squared-error as loss function tended to produce blurry and weird images rather than realistic images, since that was 'safe' choice of model under loss function that puts penalty on the pixel-wise difference of result image, not the subjective quality of that. Pix-to-Pix model solved this problematic situation by setting up discriminator to penalize the output that does not looks like true result image and succeed in generating realistic images.

Although the Pix2Pix model succeeded in producing realistic images using GAN loss, it still had the disadvantage of requiring a paired image dataset to train the model due to the supervised setup of problem. Where the \textbf{CycleGAN}~\cite{CycleGAN} emerges is in this context. CycleGAN is different from Pix-to-Pix in that it consists of two GAN models to learn mapping function between two domains of image one direciton per one model respectively. It differs in that it compares the output image with the images in the output domain while the Pix-to-Pix model only compares with the ground truth label corresponding to the input image, and that it has cycle-consistency loss term in the loss function which helps keep the contents of output image do not differ too much from those of the input image. The authors of CycleGAN successfully built models to transform horse image into zebra image, picture taken in winter into taken in summer, and realistic photo into stylized painting of famous painters.

So far we have briefly introduced the overall structure of our research. Below is a list of the small ideas that did not affect the overall flow but were included in our experiments and had effect on the result.

\textbf{LSGAN}~\cite{LSGAN} uses least square loss function to replace the original loss function of GAN. By some experiments this is known to improve the stability of GAN training and the quality of generated images. This idea was applied on the original work of CycleGAN authors and so of us.

\textbf{PatchGAN}~\cite{improved_tech_GANs} uses discriminator having receptive field of output vector smaller than the size of input images, which result in discriminator to differentiate images only using small patches of input image. This trick is known to help the generator to generate fine details in the output image. This idea was added in original work by authors of CycleGAN, too.

\textbf{U-net}~\cite{unet} is a model that is first developed for the semantic segmentation in medical image. We used this U-net structure as the generators of CycleGAN, expecting it to help keeping the details in content of input image till the output layer. More explanation on the model structure and implementation will be provided at Chapter~\ref{Ch:Model}. For those who wants more detailed information can refers to the source code on github.

\textbf{Checkerboard Artifacts} on the output image are an well-known problematic phenomenon of using transposed-convolution(or deconvolution or up-convolution). We refered to this article~\cite{odena2016deconvolution} published by Google Brain researchers and applied the main idea of replacing transposed-convolution with nearest-neighbor upsampling followed by 3 by 3 convolution to remove strange checkerboard patterns in the output images.

\textbf{Generating Sketch Image} does not requires complicated processing and can be implemented simply with image operations like gaussian blur, inverting color, add pixel values and brightness, contrast shift operation. We implementated simple version of sketch effect filters using PIL library but since this result was not so helpful to the final result, the detailed process will provided on appendix for those who are interested.

The team would like to thank \textbf{Tencent} for the generous sponsorship of this project.

\endinput




